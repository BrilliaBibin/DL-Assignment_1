{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the function of a summation junction of a neuron? What is threshold activation function?\n",
        "\n",
        "A summation junction for the input signals is weighted by the respective synaptic weight. Because it is a linear combiner or adder of the weighted input signals, the output of the summation junction can be expressed as follows: y_{sum}=\\sum_{i=1}^{n}w_ix_i\n",
        "\n",
        "A threshold activation function (or simply the activationfunction, also known as squashing function) results in anoutput signal only when an input signal exceeding a specific threshold value comes as an input. It is similar in behaviour to the biological neuron which transmits the signal only when the total input signal meets the firing threshold"
      ],
      "metadata": {
        "id": "BBZLnNg4tqw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is a step function? What is the difference of step function with threshold function?\n",
        "\n",
        "Step Function: It is a commonly used activation function. it gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Expressing it mathematically,\n",
        "\n",
        "y_{out}=f(y_{sum})=\\bigg\\{\\begin{matrix} 1, x \\geq 0 \\\\ 0, x < 0 \\end{matrix}\n",
        "\n",
        "\n",
        "The threshold function is almost like the step function, with the only difference being a fact that \\theta    is used as a threshold value instead of . Expressing mathematically,\n",
        "\n",
        "y_{out}=f(y_{sum})=\\bigg\\{\\begin{matrix} 1, x \\geq \\theta \\\\ 0, x < \\theta \\end{matrix}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BB8jj-ONz682"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the McCulloch–Pitts model of neuron.\n",
        "\n",
        "  The McCulloch-Pitts neural model, which was the earliest ANN model, has only two types of inputs — Excitatory and Inhibitory. The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. The inputs of the McCulloch-Pitts neuron could be either 0 or 1. It has a threshold function as an activation function. So, the output signal yout is 1 if the input ysum is greater than or equal to a given threshold value, else 0."
      ],
      "metadata": {
        "id": "GrF5rjMI1e5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the ADALINE network model.\n",
        "\n",
        "   Adaptive Linear Neural Element (ADALINE) is an early single-layer ANN developed by Professor Bernard Widrow of Stanford University. As depicted in the below diagram, it has only output neurons. The output value can be +1 or -1. A bias input x0 (where x0 =1) having a weight w0 is added. The activation function is such that if weighted sum is positive or 0, the output is 1, else it is -1.\n",
        "\n",
        "y_{sum} = \\sum_{i=1}^nw_ix_i+b, where\\:b = w_0\n",
        "\n",
        "y_{out}=f(y_{sum})=\\bigg\\{\\begin{matrix} 1, x \\geq 1 \\\\ -1, x < 1 \\end{matrix}"
      ],
      "metadata": {
        "id": "fAnQwzuA3bac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?\n",
        "\n",
        "  The output of a perceptron can only be a binary number (0 or 1) due\n",
        "to the hard limit transfer function. Perceptron can only be used to classify the linearly separable sets of input vectors. If input vectors are non-linear, it is not easy to classify them properly."
      ],
      "metadata": {
        "id": "1IsPwHso3wLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is linearly inseparable problem? What is the role of the hidden layer?\n",
        "\n",
        "  Clearly not all decision problems are linearly separable: they cannot\n",
        "be solved using a linear decision boundary. Problems like these are termed linearly inseparable. XOR is a linearly inseparable problem.\n",
        "\n",
        "  In neural networks, a hidden layer is located between the input and\n",
        "output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network."
      ],
      "metadata": {
        "id": "Pz38AEIer4by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain XOR problem in case of a simple perceptron.\n",
        "\n",
        "  The XOR, or \"exclusive OR\", problem is a classic problem in the field\n",
        "of artificial intelligence and machine learning. It is a problem that cannot be solved by a single layer perceptron, and therefore requires a multi-layer perceptron or a deep learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZOABG6AubOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# XOR inputs and outputs\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "outputs = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "model.add(Dense(2, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(inputs, outputs, epochs=5000, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "print(model.evaluate(inputs, outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeYUS7qCvTD3",
        "outputId": "368bfc20-198a-4a28-966b-8b1fb7815244"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0495 - accuracy: 1.0000\n",
            "[0.049477752298116684, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The XOR problem is a classic problem in artificial intelligence and\n",
        "machine learning that illustrates the limitations of single-layer perceptrons and the power of multi-layer perceptrons. By using a neural network with at least one hidden layer, it is possible to model complex, non-linear functions like XOR. This makes neural networks a powerful tool for various machine learning tasks."
      ],
      "metadata": {
        "id": "d4Me1Tn4v9TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Design a multi-layer perceptron to implement A XOR B.\n",
        "\n",
        "   To solve this problem, we add an extra layer to our vanilla\n",
        "perceptron, i.e., we create a Multi Layered Perceptron (or MLP). We call this extra layer as the Hidden layer. To build a perceptron, we first need to understand that the XOr gate can be written as a combination of AND gates, NOT gates and OR gates in the following way:\n",
        "a XOr b = (a AND NOT b)OR(bAND NOTa)\n",
        "\n",
        "  Here, we need to observe that our inputs are 0s and 1s. To make it a\n",
        "XOr gate, we will make the h1 node to perform the (x2 AND NOT x1) operation, the h2 node to perform (x1 AND NOT x2) operation and the y node to perform (h1 OR h2) operation."
      ],
      "metadata": {
        "id": "keoupqYGwCVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the single-layer feed forward architecture of ANN.\n",
        "\n",
        "  In this type of network, we have only two layers input layer and the\n",
        "output layer but the input layer does not count because no computation is performed in this layer. The output layer is formed when different weights are applied to input nodes and the cumulative effect per node is taken. After this, the neurons collectively give the output layer to compute the output signals."
      ],
      "metadata": {
        "id": "Lsa2e4imxaCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain the competitive network architecture of ANN.\n",
        "\n",
        "   A neural network consists of three layers. The first layer is the\n",
        "input layer. It contains the input neurons that send information to the hidden layer. The hidden layer performs the computations on input data and transfers the output to the output layer. It includes weight, activation function, cost function.\n",
        "\n"
      ],
      "metadata": {
        "id": "aH5IR9dfyKe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.\n",
        "\n",
        "  The backpropagation algorithm performs learning on a multilayer\n",
        "feed-forward neural network. It iteratively learns a set of weights for prediction of the class label of tuples. A multilayer feed-forward neural network consists of an input layer, one or more hidden layers, and an output layer.\n",
        "  The backpropagation algorithm works by computing the gradient of the\n",
        "loss function with respect to each weight via the chain rule, computing the gradient layer by layer, and iterating backward from the last layer to avoid redundant computation of intermediate terms in the chain rule.\n",
        "\n"
      ],
      "metadata": {
        "id": "i2UfWOwhzN-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of neural networks?\n",
        "\n",
        "Advantages of neural networks:\n",
        "\n",
        "1.Neural networks have the ability to learn on their own and generate output that is not limited to the input they provide.\n",
        "\n",
        "2.The input data is stored in its own networks instead of the database. Hence, data loss does not affect the way it operates.\n",
        "\n",
        "3.The neural network will learn from instances and adapt them when a similar event occurs, thereby allowing them to function through an event in real-time.\n",
        "\n",
        "4.Even if the neuron does not respond or information is lost, the network is still able to detect the fault and generate the output.\n",
        "\n",
        "5.Neural networks conduct multiple tasks in parallel without impacting the performance of the system.\n",
        "\n",
        "6.Storing information on the entire network.\n",
        "\n",
        "7.Ability to work with incomplete knowledge.\n",
        "\n",
        "8.Having fault tolerance.\n",
        "\n",
        "9.Having a distributed memory.\n",
        "\n",
        "10.Gradual corruption.\n",
        "\n",
        "11.Ability to make machine learning.\n",
        "\n",
        "12.Parallel processing capability.\n",
        "\n",
        "Disadvantages of neural network:\n",
        "\n",
        "1.The main disadvantages of neural networks are their black-box nature.\n",
        "\n",
        "2.Sometimes you need more control over the details of the algorithm, although there are libraries like Keras that make the development of neural networks fairly simple.\n",
        "\n",
        "3.Neural networks usually require much more data than traditional algorithms, as in at least thousands if not millions of labelled samples.\n",
        "\n",
        "4.Neural networks are also more complex in computing terms than traditional algorithms.\n",
        "\n",
        "5.The duration of the neural network is unknown.\n",
        "\n",
        "6.Hardware dependence.\n",
        "\n",
        "7.Unexplained behaviour of the network.\n",
        "\n",
        "8.Determination of proper network structure.\n",
        "\n",
        "9.The difficulty of showing the problem of the network."
      ],
      "metadata": {
        "id": "iQ1bJ1Uq3g5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write short notes on any two of the following:\n",
        "\n",
        "1. Biological neuron\n",
        "2. ReLU function\n",
        "3. Single-layer feed forward ANN\n",
        "4. Gradient descent\n",
        "5. Recurrent networks\n",
        "\n",
        "1.Biological Neural Network:\n",
        "\n",
        "  Biological Neural Network (BNN) is a structure that consists of\n",
        "Synapse, dendrites, cell body, and axon. In this neural network, the processing is carried out by neurons. Dendrites receive signals from other neurons, Soma sums all the incoming signals and axon transmits the signals to other cells.\n",
        "\n",
        "2. ReLU function:\n",
        "\n",
        "  Relu or Rectified Linear Activation Function is the most common choice\n",
        "of activation function in the world of deep learning. Relu provides state of the art results and is computationally very efficient at the same time.\n",
        "\n",
        "The basic concept of Relu activation function is as follows:\n",
        "\n",
        "Return 0 if the input is negative otherwise return the input as it is."
      ],
      "metadata": {
        "id": "2y1whMri4dvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bdIR7s-3tpOt"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "\treturn max(0.0, x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1.0\n",
        "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
        "x = -10.0\n",
        "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
        "x = 0.0\n",
        "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
        "x = 15.0\n",
        "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
        "x = -20.0\n",
        "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HJgwAHi8BJq",
        "outputId": "b74b566b-3113-4c17-c440-39cf46e88775"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying Relu on (1.0) gives 1.0\n",
            "Applying Relu on (-10.0) gives 0.0\n",
            "Applying Relu on (0.0) gives 0.0\n",
            "Applying Relu on (15.0) gives 15.0\n",
            "Applying Relu on (-20.0) gives 0.0\n"
          ]
        }
      ]
    }
  ]
}